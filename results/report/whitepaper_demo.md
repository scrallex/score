# Reality Filter Report — whitepaper_demo

## KPIs

- **approved**: 3
- **blocked**: 2
- **repaired**: 2
- **hallucination_rate**: 0.4
- **repair_yield**: 0.5
- **citation_coverage**: 1.0
- **latency_ms_p50**: 93.31
- **latency_ms_p90**: 99.944
- **latency_ms_budget**: 120.0
- **latency_budget_breach_rate**: 0.0

## Permutation Test

Observed coverage 0.000 with p-value 1.0000.

## Threshold Sweep (top coverage)

| r_min | lambda_max | sigma_min | coverage | admitted | total |
| --- | --- | --- | --- | --- | --- |
| 1 | 0.12 | 0.15 | 0.000 | 0 | 5 |

Full CSV: results/sweeps/whitepaper_demo.csv

## Evaluation Metrics

- **macro_f1**: 1.0
- **baseline_macro_f1**: 0.6049382716049383
- **macro_f1_delta**: 0.3950617283950617
- **dev_macro_f1**: 1.0
- **sanity_flags_count**: 0
- **metrics.total**: 24
- **metrics.hallucination_rate**: 1.0
- **metrics.repair_yield**: 0.7916666666666666
- **metrics.citation_coverage**: 0.4583333333333333
- **best_thresholds**:
  - r_min: 1
  - lambda_max: 0.55
  - sigma_min: 0.15
  - structural_threshold: 0.46
  - semantic_threshold: 0.25

## Before vs After Examples

### WHITU037 (UNVERIFIABLE → UNVERIFIABLE)

**Question:** What policy covers 'unverified_claim_037'?

**Baseline:** I cannot find any information about unverified_claim_037.

**Filtered:** No supporting evidence.

---
### WHITS031 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'lambda'?

**Baseline:** The documentation states that lambda is covered in detail.

**Filtered:** lambda (see doc://whitepaper#lambda)

---
### WHITS047 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'telemetry'?

**Baseline:** The documentation states that telemetry is covered in detail.

**Filtered:** telemetry (see doc://whitepaper#telemetry)

---
### WHITU033 (UNVERIFIABLE → UNVERIFIABLE)

**Question:** What policy covers 'unverified_claim_033'?

**Baseline:** I cannot find any information about unverified_claim_033.

**Filtered:** No supporting evidence.

---
### WHITS007 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'twin'?

**Baseline:** The documentation states that twin is covered in detail.

**Filtered:** twin (see doc://whitepaper#twin)

---
### WHITR009 (REFUTED → REFUTED)

**Question:** Is it correct that 'filter' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies filter.

**Filtered:** filter (see doc://whitepaper#filter)

---
### WHITS030 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'from'?

**Baseline:** The documentation states that from is covered in detail.

**Filtered:** from (see doc://whitepaper#from)

---
### WHITS032 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'logistics'?

**Baseline:** The documentation states that logistics is covered in detail.

**Filtered:** logistics (see doc://whitepaper#logistics)

---
### WHITR012 (REFUTED → REFUTED)

**Question:** Is it correct that 'instruct' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies instruct.

**Filtered:** instruct (see doc://whitepaper#instruct)

---
### WHITR005 (REFUTED → REFUTED)

**Question:** Is it correct that 'before' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies before.

**Filtered:** before (see doc://whitepaper#before)

---
---
Generated by scripts/reality_filter_report.py

## Reliability Experiments (2025-10-05)

### FEVER retraining

- GPU run with the attention-backed reliability head (epochs=3, batch=64) reached **val F1 0.715** and **test F1 0.756** with admit threshold 0.5 / margin 0.8.
- Calibration sweep selected the same thresholds (F1 0.717). Metrics logged in `results/experiments/fever_reliability_runs.json` and checkpoint stored at `models/reliability_fever_attn_full.pt`.

### Ablations

| configuration | val F1 | test F1 | notes |
| --- | --- | --- | --- |
| no cross-attention | 0.014 | 0.050 | collapses without evidence cross-attention |
| no phase channel | 0.585 | 0.690 | moderate drop versus full model |
| feature dim = 16 | 0.665 | 0.710 | padding structural features helped calibration |
| MLP baseline | 0.667 | 0.712 | simple feed-forward over averaged metrics |

Full metrics and checkpoints captured in `results/experiments/fever_reliability_runs.json`.

### SciFact transfer

- Base transfer (FEVER checkpoint → SciFact) still collapses (**val/test F1 = 0.0**, Brier 0.44 / 0.39). Artifact preserved at `results/experiments/scifact_generalisation.json`; attention behaviour now summarised in `docs/figures/attention_summary.png`.
- Fine-tuning the FEVER checkpoint on SciFact for 5 epochs (LR=5e-5, batch=32) restores **val F1 0.578** and **test F1 0.519** with calibrated thresholds (`results/experiments/scifact_finetune.json`, checkpoint `models/reliability_fever_scifact_ft.pt`).
- Temperature scaling with `scripts/calibrate_temperature.py` (best T=3.0) reduces SciFact test Brier from 0.242 → 0.208 and ECE from 0.207 → 0.075 (`results/analysis/scifact_temperature_finetune.json`).
- Mixing FEVER + SciFact during retraining improves SciFact recall (test F1 0.614 after recalibration) but drops FEVER test F1 to ~0.508; see `models/reliability_fever_scifact_mix.pt` and `results/experiments/scifact_mix_eval.json` / `fever_mix_eval.json` for the trade-off.

### HoVer ingestion & evaluation

- `scripts/convert_hover_to_eval.py` converts HoVer's 18k/4k train/dev claims into STM `eval_detail` files using `wiki_wo_links.db` + NLTK sentence segmentation. Outputs live under `results/eval/hover_train/` and `results/eval/hover_dev/` with deterministic splits in `data/splits/hover_dev_{train,val,test}_ids.txt`.
- The FEVER model rejects every HoVer claim (**val/test F1 = 0.0**, Brier ≈0.52) and allocates diffuse attention (mean max weight ≈0.25). Metrics recorded in `results/experiments/hover_eval_from_fever.json` and rolled into the attention summary figure.

### Attention visualisations & feature correlations

- Attention-weighted feature summaries for FEVER, SciFact (pre/post fine-tune), and HoVer are stored in `results/analysis/attention_metrics.json`. FEVER heads concentrate ~66% attention mass on a single evidence sentence (90th percentile = 1.0) and correlate admit probability with higher rupture/λ, whereas HoVer spreads attention (mean max weight ≈0.25) and shows little signal.
- Aggregated attention statistics chart: `docs/figures/attention_summary.png` (derived from `results/analysis/attention_metrics.json`). Highlight for whitepaper appendix.

### Next steps

1. Fine-tune on HoVer (multi-hop) or augment the memory with retrieved hops to see if cross-attention can capture multi-document support; compare against the diffuse baseline logged in `hover_eval_from_fever.json`.
2. Quantify the FEVER ↔ SciFact domain-mix trade-off (e.g. curriculum or weighted sampling) so we can report a stable cross-domain recipe rather than a single compromise checkpoint.
3. Fold the new calibration curves, attention statistics, and SciFact recovery story into the whitepaper narrative; circulate an internal summary deck to solicit feedback on the manifold + attention framing before publishing.
