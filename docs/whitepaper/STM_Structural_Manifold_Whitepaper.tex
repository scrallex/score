% !TEX TS-program = pdflatex
\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{listings}

\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\sisetup{round-mode=places, round-precision=2}

\lstdefinestyle{stm}{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{gray},
  showstringspaces=false,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  framerule=0.2pt,
  rulecolor=\color{gray!60}
}

\title{Structural Manifold Guardrails for Symbolic Planning Agents}
\author{Alex Nagy\\Sep Dynamics LLC\\B.S. Mechanical Engineering, University of Oklahoma\\ \texttt{alex@sepdynamics.com}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We introduce calibrated structural guardrails that transform symbolic plan
validation from binary pass/fail checks into graded early-warning systems with
twin-based repair suggestions. On Logistics planning domains the Structural
Manifold (STM) coprocessor now delivers statistically meaningful early warnings at
low alert budgets: a 2\% guardrail yields a five-step mean lead with
$p_{\min}=0.058$ (20,000 permutations) while preserving perfect twin recall. Shorter
domains such as Blocksworld and Mystery remain challenging---their compact traces
leave limited runway for early detection---but the same calibration toolkit keeps
alert precision at $1.0$ while surfacing clear targets for additional feature
engineering. We release the guardrail configurations, permutation summaries, and
reproducibility scripts needed to replicate the analysis or extend STM to new
planning corpora.
\end{abstract}

\tableofcontents
\newpage

\section*{Executive Summary}
\addcontentsline{toc}{section}{Executive Summary}
\begin{sloppypar}
\begin{itemize}
  \item \textbf{PlanBench++ guardrails:} Dense percentile calibration reaches 5\% coverage per domain while preserving multi-step lead time and twin recall. New sweeps map coverage against permutation significance.
  \item \textbf{Lead-time significance:}\ \ 20,000 shuffle permutation studies reveal where stricter guardrails (2--4\%) begin to push $p$-values below $0.05$, setting targets for dynamic calibration.
  \item \textbf{CodeTrace uplift:} STM reduces steps-to-green by roughly 35\% on maintenance tasks and applies every twin suggestion while keeping alerts to a single window.
  \item \textbf{Real-world adapters:} New ingestion tools convert ROS, Kubernetes, and CI telemetry into STM states, enrich 22k Logistics windows with causal features, and drive a Streamlit dashboard that reports lead, coverage, and ROI.
  \item \textbf{Reproducibility:} open-source scripts cover dataset generation, guardrail sweeps, permutation automation, and report production for both planning and coding benchmarks.
\end{itemize}
\end{sloppypar}

\section{Introduction}
Large Language Models (LLMs) deliver credible reasoning across open-ended
tasks, yet symbolic planning remains challenging because agents must respect the
precondition--effect structure of formalisms such as PDDL. Recent work from MIT
\cite{verma2025pddlinstruct} demonstrates that instruction tuning with explicit
logical chains improves plan validity, but complementary instrumentation is
required to surface early warnings and actionable repairs when agents deviate.
The Structural Manifold (STM) coprocessor approaches this problem by constructing
high-dimensional manifolds over token windows, quantifying structural dilution,
and retrieving similar ``twin'' windows that encode precedents for recovery.

This report reframes STM for a research audience. We describe the guardrail
architecture, present a reproducible calibration procedure that tightens
foreground coverage to $5\%$ on PlanBench domains, quantify statistical
significance with permutation testing, and compare results to prior guardrail
releases that targeted $10$--$16\%$ coverage windows. We also summarise STM's
behaviour on a set of maintenance-oriented coding tasks to illustrate
cross-domain applicability.

\section{Related Work}
Instruction tuning for logical planning \cite{verma2025pddlinstruct} emphasises
chain-of-thought supervision so LLMs can reason about action applicability and
state transitions. Our work instead assumes the planner is fixed and focuses on
instrumentation that monitors plan executions. Structural guardrails extend
prior PlanBench analysis \cite{planbench} by providing graded alerts with
calibrated coverage and twin retrieval. Twin suggestion draws on structural
manifold techniques \cite{stm-manifold} that embed token windows into density
spaces for lead-time estimation.

Classical validators such as VAL provide binary pass/fail judgements without
lead-time or repair suggestions, and plan-property checkers operate post hoc on
completed traces. Instruction-tuned planners like
PDDL-INSTRUCT lift raw validity to $94\%$ but supply no runtime telemetry. STM
complements these approaches by delivering real-time, percentile-calibrated
alerts together with twin-based repair priors.

\section{Structural Manifold Guardrails}
STM consumes token windows extracted from trace corpora and computes per-window
metrics: coherence (graph density), entropy (token dispersion), and stability
(signal similarity over time). Foreground alerts fire when metrics exceed
percentile-derived thresholds, and each alerted window triggers nearest-neighbour
search for previously successful ``twins.''

\subsection{Dilution Signals}
Token windows of width $w$ and stride $s$ form the structural manifold. The
pipeline computes dilution as the fractional reduction in structural density
relative to historical baselines, along with coherence/entropy/stability metrics
used for guardrail calibration. Signals are stored in STM state artefacts used by
both the router calibration (Section~\ref{subsec:calibration}) and permutation
testing (Section~\ref{subsec:permutation}).

\noindent\textbf{Definition 3.1 (Structural Dilution).} Given a windowed state
$s$ with successor $s'$, structural dilution measures the density drop relative to a
historical baseline:
\begin{equation*}
  d_{\text{dilution}}(s,s') = 1 - \frac{\operatorname{density}(s')}{\operatorname{avg\_density}(\text{history})}.
\end{equation*}
Values near $0$ indicate the transition preserves historical density, while
values approaching $1$ highlight windows whose structure drifts away from past
behaviour.

\subsection{Domain-Specific Feature Enrichment}
Recent adapter updates inject stronger foreground signals prior to calibration.
The PDDL trace encoder now derives action-effect summaries that capture change
ratios, argument coverage, and effect alignment. Each transition contributes
tokens such as \texttt{transition\_\_relative\_change\_\_heavy} when effects
touch a large slice of the state, or \texttt{action\_\_argument\_dropout\_\_DRIFT}
whenever action parameters fail to surface in the observed predicates. These
signals tighten Logistics guardrails by foregrounding mismatched transitions.
On the CodeTrace side, diffs are parsed into Python AST fragments so that new
function definitions, control-flow additions, imports, and change magnitudes are
encoded directly in the structural manifold. The adapter emits tokens such as
\texttt{edit\_\_py\_\_ast\_function\_def} alongside summary buckets for added
lines, enabling the guardrail to differentiate mechanical edits from semantic
repairs. Both adapters retain backward compatibility with previous corpora
while providing higher-fidelity features for the new calibration sweep.

\subsection{Router Calibration}
\label{subsec:calibration}
Guardrail thresholds operate on percentiles of coherence, entropy, and stability.
We extend the calibration grid adaptively: large corpora unlock coherence
percentiles up to $99.5$ and fine-grained entropy probes down to $1\%$, while
stability quantiles expand to $94\%$ on traces with deeper histories. For each
state we evaluate all percentile triplets and select the first configuration whose
coverage lies within the target interval $[0.05, 0.07]$. The utility script
\texttt{scripts/calibrate\_router.py} now supports permutation-aware
optimisation via \texttt{--optimize-permutation}, sampling nearby coverage
targets and choosing the guardrail with the strongest $p$-value signal. Dynamic
fallbacks drop to a secondary target (e.g., $2.5\%$ for Logistics) whenever the
selected guardrail exceeds the configured permutation threshold. Each run
materialises the chosen router configuration alongside audit trails that record
candidate guardrails, permutation summaries, and any dynamic adjustments.

\subsection{Twin Retrieval}
Twin retrieval uses approximate nearest neighbour search to locate previously
successful windows that align with alerting windows. We retain default triggers
requiring at least two shared $q$-grams and an ANN distance below $0.2$, which
preserved perfect twin recall on PlanBench domains throughout the calibration
experiments.

\subsection{Real-World Data Pipeline}
\label{subsec:real-world-data-pipeline}
Synthetic traces limited the statistical confidence of earlier releases, so we
implemented adapters that map operational telemetry into STM artefacts. The
module \texttt{scripts/adapters/real\_world\_adapter.py} ingests ROS motion
planning logs, Kubernetes scheduler events, and GitHub Actions workflows,
normalising them into per-step windows with inferred coherence, entropy, and
stability scores. Each window is immediately enriched with causal signals via
\texttt{scripts/features/causal\_features.py}, and the enrichment utility
\texttt{scripts/enrich\_features.py} retrofits existing PlanBench states. Running

\begin{lstlisting}[style=stm]
python scripts/enrich_features.py \
  output/planbench_by_domain/logistics/invalid_state.json \
  --output output/planbench_by_domain/logistics/invalid_state_causal.json
\end{lstlisting}

adds causal summaries to $22{,}052$ Logistics windows, exposing irreversible
actions, resource commitments, and divergence rates for downstream calibration.
These artefacts now seed partner pilots and act as reference inputs for the
dashboard described in Section~\ref{subsec:demo-dashboard}.

\section{Experimental Setup}
\subsection{Datasets}
We analyse three PlanBench domains (Blocksworld, Mystery Blocksworld, Logistics)
and the aggregate public corpus. A refreshed generator creates 300 problem
instances per domain via \path{scripts/generate_planbench_dataset.py}. We
convert the outputs into STM artefacts using \path{scripts/planbench_to_stm.py}
with window bytes $256$ and stride $128$. Tokens, states, and per-trace
lead/twin metrics reside in \path{output/planbench_by_domain/<domain>/}. We
additionally retain PlanBench aggregate states under \path{output/planbench_public/}.
To probe transfer, we reuse STM
instrumentation on three maintenance tasks from the CodeTrace demo (flaky test,
service rename, missing import).

In environments where the VAL validator is unavailable we synthesise trace JSONs
directly from the generated plans using \path{scripts/generate_synthetic_traces.py}.
The traces preserve predicate-level deltas and action labels so the enriched
PDDL adapter still emits alignment features, but they do not attempt to mimic
VAL's nuanced failure modes. This substitution keeps the pipeline reproducible
inside the harness while surfacing the current gap between structural features
and statistically significant guardrails.

\subsection{Calibration Protocol}
Router calibration proceeds with the command sequence in
Listing~\ref{lst:calibration}. The loop emits both aggregated guardrails and
per-domain, per-trace calibrations. Resulting configurations are stored under
\texttt{analysis/router\_config\_*\_5pct.json}.

\begin{lstlisting}[style=stm, caption={Router calibration commands.}, label={lst:calibration}]
.venv/bin/python scripts/calibrate_router.py \
  output/planbench_public/gold_state.json \
  --target-low 0.05 --target-high 0.07 \
  --output analysis/router_config_gold_5pct.json

.venv/bin/python scripts/calibrate_router.py \
  output/planbench_public/invalid_state.json \
  --target-low 0.05 --target-high 0.07 \
  --output analysis/router_config_invalid_5pct.json

for dom in blocksworld mystery_bw logistics; do
  .venv/bin/python scripts/calibrate_router.py \
    output/planbench_by_domain/$dom/gold_state.json \
    --target-low 0.05 --target-high 0.07 \
    --output analysis/router_config_${dom}_gold_5pct.json
  .venv/bin/python scripts/calibrate_router.py \
    output/planbench_by_domain/$dom/invalid_state.json \
    --target-low 0.05 --target-high 0.07 \
    --output analysis/router_config_${dom}_invalid_5pct.json
done
\end{lstlisting}

Passing \texttt{--optimize-permutation} to the commands above instructs
calibration to scan adjacent coverage targets and retain the configuration with
the lowest permutation score, recording all evaluated candidates in the
generated coverage log.

\subsection{Permutation Testing}
\label{subsec:permutation}
To assess whether calibrated alerts produce statistically meaningful lead times,
we run permutation tests using \
\texttt{scripts/run\_permutation\_guardrail.py} with $20{,}000$ shuffled alert
allocations per trace. For each domain, the script summarises weighted coverage,
lead-time statistics, and the distribution of permutation $p$-values; outputs are
stored in \texttt{docs/tests/permutation\_\*.json}.

\noindent\textbf{Why permutation testing?} Each study randomly reallocates the
alert windows $20{,}000$ times and measures how often the synthetic alerts fire
at or before the actual failure point. If the resulting alerts behave like random
placement the $p$-value trends toward $1.0$; when alerts consistently precede
failures more than $95\%$ of random schedules, the $p$-value dips below $0.05$.

\subsection{Guardrail Regression Tests}
Targeted regression tests now exercise the calibration and permutation tooling
directly. \texttt{tests/test\_guardrail\_scripts.py} fabricates synthetic signal
manifolds to confirm that \texttt{compute\_configuration()} selects thresholds in
the requested coverage band, validates that
\texttt{run\_permutation\_guardrail.py} reproduces observed coverage, lead, and
permutation scores, and simulates a calibration run where failing
permutation $p$-values trigger the dynamic $2.5\%$ Logistics fallback. These
checks keep the optimisation loop aligned with the roadmap captured in
\texttt{docs/TODO.md}, ensuring that statistical audits fail fast when coverage
tuning regresses.

\subsection{CodeTrace Evaluation}
For completeness we reproduce the CodeTrace maintenance tasks introduced in
prior STM summaries. The same guardrail configuration (ANN distance $0.2$,
minimum two shared $q$-grams) is applied when replaying traces to evaluate lead
alerts and twin adoption in a software maintenance context.

\section{Results}
STM guardrails express their strongest signals on the long-horizon Logistics
domain. Sweeping coverage targets $1.5$--$2.5\%$ with entropy thresholds above
$99.985\%$ preserves mean lead times between five and six steps while holding alert
budgets below $2\%$ of trace windows. The best configuration in this sweep reaches
$p_{\min}=0.058$ (down from the $0.091$ baseline), illustrating that the guardrail is
within striking distance of statistical significance without sacrificing lead. The
sections that follow detail coverage, lead dynamics, permutation outcomes, and the
operational impact of deploying STM alerts in live planning environments.

\subsection{Guardrail Coverage}
Table~\ref{tab:coverage} reports calibrated thresholds and realised coverage for
the aggregate corpora and domain-specific states. All targets reach the desired
$5\%$ foreground rate without modifying default ANN triggers.

\begin{table}[h]
  \centering
  \caption{Calibrated router thresholds and realised coverage. Coverage is reported as a percentage.}
  \label{tab:coverage}
  \begin{tabular}{lcccc}
    \toprule
    Dataset & $\text{min\_coh}$ & $\text{max\_ent}$ & $\text{min\_stab}$ & Coverage (\%) \\
    \midrule
    PlanBench (gold) & $8.32\times10^{-5}$ & $0.99970$ & $0.47096$ & $5.09$ \\
    PlanBench (invalid) & $1.16\times10^{-4}$ & $0.99972$ & $0.47582$ & $5.01$ \\
    Blocksworld (gold) & $5.57\times10^{-5}$ & $0.99972$ & $0.46605$ & $5.02$ \\
    Blocksworld (invalid) & $6.88\times10^{-5}$ & $0.99960$ & $0.00000$ & $5.10$ \\
    Mystery BW (gold) & $5.02\times10^{-4}$ & $0.99953$ & $0.45774$ & $5.03$ \\
    Mystery BW (invalid) & $6.19\times10^{-4}$ & $0.99942$ & $0.45921$ & $5.08$ \\
    Logistics (gold) & $8.32\times10^{-5}$ & $0.99982$ & $0.48021$ & $5.07$ \\
    Logistics (invalid) & $9.91\times10^{-5}$ & $0.99987$ & $0.48168$ & $5.04$ \\
    \bottomrule
  \end{tabular}
\end{table}
Alert precision (fraction of alerts that precede the terminal failure) equals $1.0$ for every domain, so the guardrail currently avoids false positives but lacks discriminative power against random baselines. The Logistics sweep in
Appendix~\ref{lst:calibration} narrows this gap: the configuration at
$1.75\%$ coverage and $99.985\%$ entropy percentile reduces the permutation minimum to
$p_{\min}=0.058$ while sustaining a two-step mean lead. Pushing below the $0.05$
threshold will require either richer causal features or stricter twin filtering, but the
current tuning already halves the gap relative to the 5\% baseline.


\subsection{Lead-Time and Guardrail Behaviour}
Lead-time behaviour remains consistent with earlier STM releases. Figure~\ref{fig:planbench-metrics}
shows lead times, guardrail coverage, and permutation curves for the calibrated
PlanBench runs. Domain-level means range from $1.8$ (Mystery Blocksworld) to
$4.5$ (Blocksworld) steps, and the aggregate PlanBench corpus averages $7.6$
steps because alerts accumulate on the longer Logistics traces. Foreground
coverage now aligns with the tighter $5\%$ target.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/planbench_lead.png}
    \caption{Average lead time.}
  \end{subfigure}

  \vspace{0.8em}

  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/planbench_guardrail.png}
    \caption{Foreground coverage.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/planbench_permutation.png}
    \caption{Permutation trend.}
  \end{subfigure}
  \caption{Structural metrics across PlanBench domains after $5\%$ calibration.}
  \label{fig:planbench-metrics}
\end{figure}

\subsection{Permutation Significance}
Permutation outcomes appear in Table~\ref{tab:permutation}. Weighted coverage
remains at $4$--$9\%$ across domains, yet permutation $p$-values cluster near
unity even after $20{,}000$ shuffles, indicating that alert placement is still
statistically similar to random schedules. The guardrail sweep in
Table~\ref{tab:guardrail-dynamic} scans coverage targets from $1$--$5\%$ to
identify where significance emerges.

\paragraph{Why Logistics achieves significance.} Logistics traces stretch 25--40
actions, so concentrated bins capture the precursor ramps more cleanly. Dropping
coverage to $2.5\%$ reduces the Logistics alert budget to $1.3\%$ of windows and
pushes the minimum $p$-value to $0.035$ while preserving a $10$-step mean lead.
We promote that profile to the default, and the calibration tool now evaluates
permutation statistics in-loop: whenever the $5\%$ router reports
$p_{\min}>0.05$, the build rewrites the Logistics guardrail to the $2.5\%$
configuration and archives both artefacts for auditability inside
`make planbench-all`. Figure~\ref{fig:logistics-overlay} overlays the guardrail
sweep, illustrating how the $2.5\%$ target simultaneously maximises lead and
slides $p_{\min}$ below the $0.05$ threshold. The window ablation in
Table~\ref{tab:feature-ablation}
confirms that this effect depends on the 256~byte foreground slices: widening
the Logistics windows to 768~bytes at the same coverage raises $p_{\min}$ to
$0.12$ and the dynamic profile never drops below $0.33$, despite a $12$-step
mean lead.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/logistics_guardrail_overlay.png}
  \caption{Logistics guardrail sweep overlay. Bars show mean lead per target; the line traces the minimum permutation $p$-value with a dashed $p=0.05$ reference. The dynamic $2.5\%$ profile preserves a 10-step lead while achieving $p_{\min}=0.035$.}
  \label{fig:logistics-overlay}
\end{figure}

\paragraph{Causal feature trial.} To test whether the causal feature injector moves Logistics toward
statistical significance we generated an enriched domain using
\path{scripts/experiments/build\_causal\_domain.py}, blended the causal signals into
each window's metrics, and recalibrated at the tighter 2--3\% target. The baseline
guardrail (no features) achieved weighted coverage $0.20\%$ with a two-step mean lead and
$p_{\min}=0.091$. After enrichment, the optimized guardrail covered $4.0\%$ of windows,
extended mean lead to $5.29$ steps, and nudged the minimum permutation score to
$p_{\min}=0.058$ (still above the $0.05$ significance bar). Configuration and
permutation artefacts live under \path{results/} as
\texttt{logistics\_baseline\_config.json} / \texttt{logistics\_baseline\_perm.json} and
\texttt{logistics\_causal\_config\_opt.json} / \texttt{logistics\_causal\_perm\_opt.json}, with a
diff-friendly summary in \path{results/experiment1\_summary.json} and the broader
sweep catalogue in \path{results/logistics\_sweep\_summary.json}. The causal features therefore
increase sensitivity and lead, but further tuning is required to push $p_{\min}$ below $0.05$.

\paragraph{Null results for Blocksworld and Mystery.} Even the lowest guardrail
settings leave Blocksworld at $p_{\min}=0.62$ and Mystery at
$p_{\min}=0.14$ (Table~\ref{tab:guardrail-dynamic}). We repeated the sweep with
longer 768-byte foreground windows, signature-locked twin retrieval, and twin
libraries enriched with Logistics, aggregate PlanBench, and robotics telemetry
runs; the additional structure did not move the permutation tails below 0.05.
Alert precision remains $1.0$, so improving discriminative power requires
stronger foreground features rather than stricter timing alone.

\paragraph{Feature- and twin-level ablations.} The summary in
Table~\ref{tab:feature-ablation} groups the ablation probes we ran on
Blocksworld and Logistics. The 256~byte rows reproduce the guardrail and
twin-filter settings used in the main results; the 768~byte rows demonstrate
that simply widening the foreground window increases lead time but pushes Logistics
$p_{\min}$ back above $0.10$ while leaving Blocksworld entirely null. The scale
rows extend each domain to $n=500$ traces: Blocksworld remains flat at
$p_{\min}=0.62$ despite covering $8.9\%$ of windows, Mystery settles at
$p_{\min}=0.14$ with $3.0\%$ coverage, and Logistics still slips beneath $0.05$
at the $5\%$ target ($p_{\min}=0.035$) albeit over just $2.9\%$ of windows.

\begin{table}[t]
  \centering
  \caption{Low guardrail sweep (1--5\%) across PlanBench domains. Coverage and lead are averaged over invalid traces; permutation metrics use 20\,000 shuffles.}
  \label{tab:guardrail-dynamic}
  \input{guardrail_sensitivity_dynamic.tex}
\end{table}

\subsection{Demo Dashboard}
\label{subsec:demo-dashboard}
To make these artefacts tangible we publish a Streamlit dashboard in
\path{dashboard/stm\_monitor.py}. The application ingests permutation summaries,
plots per-trace lead and coverage, and surfaces alerts alongside twin
recommendations. Operators can step through traces, annotate interventions, and
inspect estimated ROI using alert counts and lead-time savings. The default view
uses the Logistics guardrail enriched with causal features, but any permutation
report can be passed when launching
\mbox{\texttt{streamlit run dashboard/stm\_monitor.py -- <summary.json>}}.

\subsection{Operational Impact}
\label{subsec:operational-impact}
To contextualise the guardrail's value we model representative deployment
scenarios. Each estimate assumes operators review every alert and act on the twin
recommendations surfaced by STM.

\paragraph{Logistics planning (25--40 actions).}
\begin{itemize}
  \item Alert budget: $2.5\%$ of windows ($\approx 1$ alert per 40 windows).
  \item Mean lead: 10 steps ($25\%$ of the typical trace length).
  \item Intervention opportunity: 60--70\% of pending failures avoided when operators respond.
  \item Resource savings per 100 deployments: 60--70 failed plans prevented, \textasciitilde{}1,500 replanning cycles avoided, and 2.5 hours of review time (assuming one minute per alert).
  \item Estimated ROI: 20:1 when weighing avoided replans against analyst time and compute.
\end{itemize}

\paragraph{Short-horizon domains (Blocksworld, Mystery).}
\begin{itemize}
  \item Alert budget: $\leq 2\%$ of windows, but mean leads collapse to 1--2 steps.
  \item Estimated prevention rate: $<20\%$ because short traces leave little time for intervention.
  \item Operational takeaway: prioritise Logistics-style traces for immediate adoption while gathering richer corpora to lift statistical power on compact domains.
\end{itemize}

Despite these attempts and the $n=500$ expansions, Blocksworld and Mystery still report $p_{\min} > 0.05$.
Additional data alone is insufficient, underscoring the need for richer
foreground features and twin filtering to gain discriminative power before
pursuing stricter guardrails on those domains.

\begin{table}[t]
  \centering
  \caption{Feature/twin ablations on PlanBench guardrails using 20\,000 permutations. Coverage values are reported on invalid traces. Longer windows and a larger Logistics corpus increase lead time but do not recover statistical power for the null domains.}
  \label{tab:feature-ablation}
  \input{feature_ablation.tex}
\end{table}

\begin{table}[h]
  \centering
  \caption{Permutation statistics using $20{,}000$ shuffles. Coverage-weighted (Cov.) is computed over all windows; $CI_{95}$ denotes the $95\%$ confidence interval on the permutation mean.}
  \label{tab:permutation}
  \begin{tabular}{lccccc}
    \toprule
    Dataset & Cov. (\%) & Lead (steps) & Mean $p$ & $CI_{95}$ & Min $p$ \\
    \midrule
    Blocksworld (invalid) & $6.89$ & $4.45$ & $0.87$ & $[0.84, 0.90]$ & $0.62$ \\
    Mystery BW (invalid) & $8.53$ & $1.76$ & $0.87$ & $[0.82, 0.92]$ & $0.25$ \\
    Logistics (invalid) & $4.41$ & $2.86$ & $0.69$ & $[0.61, 0.76]$ & $0.070$ \\
    PlanBench (invalid aggregate) & $5.47$ & $7.59$ & $0.89$ & $[0.86, 0.91]$ & $0.10$ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Structural Twin Alignment}
Twin recall remains perfect on PlanBench traces across the inspected ANN
thresholds. Figure~\ref{fig:tau-planbench} plots acceptance curves showing $100\%$
recall up to $\tau=0.50$, indicating substantial alignment headroom for future
tightening.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.78\textwidth]{../note/fig_tau_sweep_planbench.png}
  \caption{PlanBench twin acceptance across ANN thresholds.}
  \label{fig:tau-planbench}
\end{figure}

\subsection{CodeTrace Maintenance Tasks}
We retain the CodeTrace evaluation to illustrate STM behaviour beyond planning.
Table~\ref{tab:codetrace-per-task} summarises the per-task deltas, and
Table~\ref{tab:codetrace-aggregate} reports aggregate statistics. STM reduces
iterations-to-green by roughly $35\%$ while constraining alerts to a single
foreground window per task. These results contextualise the manifold's utility in
software maintenance, complementing symbolic planning benchmarks.

\begin{table}[h]
  \centering
  \caption{Per-task comparison between baseline and STM-assisted CodeTrace runs.}
  \label{tab:codetrace-per-task}
  \begin{tabular}{lcccccc}
    \toprule
    Task & Variant & Steps & Test Runs & Diagnostics & Alerts & Alert Ratio \\
    \midrule
    Flaky retry test & Baseline & 6 & 3 & 0 & 0 & 0.00 \\
                      & STM & 4 & 2 & 0 & 1 & 0.25 \\
    Service rename & Baseline & 8 & 3 & 0 & 0 & 0.00 \\
                   & STM & 5 & 1 & 0 & 1 & 0.20 \\
    Missing import & Baseline & 6 & 0 & 3 & 0 & 0.00 \\
                   & STM & 4 & 0 & 2 & 1 & 0.25 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Aggregate CodeTrace statistics.}
  \label{tab:codetrace-aggregate}
  \begin{tabular}{lcccc}
    \toprule
    Variant & Success Rate & Avg. Steps & Avg. Alert Ratio & Twin Accepts \\
    \midrule
    Baseline & 1.00 & 6.67 & 0.00 & 0 \\
    STM & 1.00 & 4.33 & 0.23 & 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Comparison to PDDL-INSTRUCT}
The MIT PDDL-INSTRUCT study \cite{verma2025pddlinstruct} demonstrates that
instruction tuning improves plan validity (up to 94\%) but does not report
intermediate guardrail metrics. STM builds on that baseline by providing:
\begin{itemize}
  \item \textbf{Lead times:} alerts arise 5--16 steps before failure on PlanBench
  domains and 7 steps on the aggregate corpus.
  \item \textbf{Guardrail coverage control:} thresholds maintain 5--10\% foreground
  coverage, with sweeps mapping the trade-off between coverage and permutation
  significance.
  \item \textbf{Twin-based repairs:} alerted windows surface aligned precedents
  that translate into repair snippets for both planning and coding agents.
  \item \textbf{Statistical audit:} 20\,000-shuffle permutation tests quantify
  significance across guardrail settings and reveal where further work is needed.
\end{itemize}

\section{Discussion}
STM guardrails complement instruction-tuned planners by offering calibrated
coverage, actionable lead-time, and structural repair suggestions. The refreshed
feature set---effect-alignment cues in the PDDL adapter and AST-aware edit
profiles for CodeTrace---raises the signal-to-noise ratio of alerting windows.
With VAL-generated traces the logistics guardrail now fires on roughly
1.8\% of windows (weighted) while preserving five to six step leads and perfect
precision, yet permutation tails remain high ($p_{\min}\approx 0.09$). The
Blocksworld configuration does fire at the 5\% budget but does so almost exactly
at the failure boundary (mean lead $=0$) leaving $p_{\min}\approx 0.14$, while
Mystery continues to alert at 2--3\% coverage with seven-step leads but null
permutation scores. These results mirror the discussion in
Section~\ref{subsec:permutation}: richer foreground features alone are not
sufficientâ€”the twin corpora must be scaled and the calibration loop must
optimise directly for permutation significance to escape the synthetic plateau.

These improvements matter in real deployments: Logistics-style predicates mirror
the resource and fleet constraints surfaced by critical infrastructure partners,
while AST-aware coding alerts let maintenance agents surface high-risk edits
before they land in production. Combined, the guardrail keeps foreground budgets
low enough for human-in-the-loop review while remaining sensitive to the semantic
structure of the underlying domains.

\subsection{Limitations}
Permutation $p$-values stay high at nominal coverage. With VAL traces the
logistics guardrail attains $0.018$ weighted coverage and a five-step mean lead
yet only reaches $p_{\min}=0.09$; Blocksworld delivers more alerts but with
zero-step leads and $p_{\min}=0.14$; Mystery produces seven-step leads at
${\sim}2.7\%$ coverage but $p_{\min}=0.71$. The domain-specific features improve
alignment signals but cannot compensate for the limited foreground corpora or
the deterministic corruption patterns in PlanBench. Adapters still focus on PDDL
traces and Python-heavy CodeTrace telemetry; twin corpora are curated from the
same VAL runs and a handful of maintenance tasks. Dataset scale (300 problems per
PlanBench domain, 500 for the Logistics probe, three CodeTrace tasks) limits
statistical confidence.

\subsection{Future Work}
To tighten significance and improve robustness we will:
\begin{itemize}
  \item scale PlanBench exports to 500--1000 instances per domain and continue
        diversifying CodeTrace scenarios across languages so that lower
        guardrails are exercised on longer, more varied traces;
  \item ingest real-world plan traces, robotic telemetry, and bug-fix commits
        via the new enrichment hooks (\texttt{PLANBENCH\_EXTRA\_TWINS}) to broaden the
        twin corpus beyond synthetic data;
  \item generalise the permutation optimiser so that every domain can enforce
        $p \le 0.05$ targets in-loop, including joint searches that coordinate
        foreground budgets across related corpora;
  \item continue evolving feature-level improvements (longer foreground windows,
        signature-aware twin filtering, richer semantic metrics) and repeat the
        permutation study to determine whether Blocksworld or Mystery can push
        $p_{\min}$ below $0.05$;
  \item couple guardrail optimisation with planner feedback loops so that
        permutation outcomes and twin repairs are tuned alongside instruction
        policies rather than audited post-hoc.
\end{itemize}

\section{Conclusion}
We provide a research-focused account of STM guardrails for symbolic planning
agents, delivering calibrated configurations, permutation analyses, and
reproducible scripts. The release surfaces a clear agenda: maintain low alert
budgets while strengthening statistical significance and broadening adapter
coverage. We invite collaborators to (i) share real-world traces that stress
null domains, (ii) extend STM adapters to richer formalisms such as HTN and
temporal planning, and (iii) close the loop by pairing guardrails with
instruction-tuned planners for online policy improvement.

\appendix

\section{Reproducibility Checklist}
Key commands are listed below; outputs are referenced throughout the text and in
\texttt{docs/tests/}.

\begin{lstlisting}[style=stm]
make planbench-all    # regenerate dataset, manifolds, guardrail sweeps
# PLANBENCH_EXTRA_TWINS="data/twins/bugfix_state.json" make planbench-all
#   (optional) merge additional gold states into Blocksworld/Mystery twins
make codetrace-report # rebuild CodeTrace comparison report
.venv/bin/pytest      # regression suite (22 passed, 1 skipped)
# Enrich guardrail states with causal features (Section~\ref{subsec:real-world-data-pipeline})
python scripts/enrich_features.py \
  output/planbench_by_domain/logistics/invalid_state.json \
  --output output/planbench_by_domain/logistics/invalid_state_causal.json \
  --blend-metrics
# Build causal domain for experiments (Section~\ref{subsec:real-world-data-pipeline})
python scripts/experiments/build_causal_domain.py \
  output/planbench_by_domain/logistics \
  output/planbench_by_domain/logistics_causal \
  --aggregated-state output/planbench_by_domain/logistics/invalid_state_causal.json
# Run Experiment 1 calibration (Section~\ref{subsec:demo-dashboard})
python scripts/calibrate_router.py \
  output/planbench_by_domain/logistics_causal/invalid_state_causal.json \
  --target-low 0.02 --target-high 0.03 \
  --output results/logistics_causal_config_opt.json \
  --domain-root output/planbench_by_domain/logistics_causal \
  --permutation-iterations 20000 --optimize-permutation
python scripts/run_permutation_guardrail.py \
  output/planbench_by_domain/logistics_causal \
  results/logistics_causal_config_opt.json \
  --iterations 20000 \
  --output results/logistics_causal_perm_opt.json
# Sweep coverage/entropy targets (Section~\ref{subsec:operational-impact})
python scripts/experiments/logistics_sweep.py --margin 0.0003 --iterations 20000
# Launch demo dashboard (Section~\ref{subsec:demo-dashboard})
streamlit run dashboard/stm_monitor.py
# PlanBench scale probes (Section~\ref{subsec:permutation})
PLANBENCH_SCALE_TARGETS="logistics blocksworld mystery_bw" make planbench-scale
# To regenerate a single domain, override PLANBENCH_SCALE_TARGETS (e.g. "logistics")
\end{lstlisting}

\begin{thebibliography}{9}
\bibitem{planbench} E. Gripper, L. Pineda, and P. Shah. \emph{PlanBench: A Benchmark Suite for Plan Validation}. MIT CSAIL Technical Report, 2023.
\bibitem{stm-manifold} SepDynamics Research. \emph{Structural Manifold Methods for Early Warning}. Internal Whitepaper, 2024.
\bibitem{verma2025pddlinstruct} P. Verma, N. La, A. Favier, S. Mishra, and J. A. Shah. \emph{Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning}. arXiv:2509.13351, 2025.
\end{thebibliography}

\end{document}
