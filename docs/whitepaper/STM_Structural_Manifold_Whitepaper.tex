% !TEX TS-program = pdflatex
\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{array}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{listings}

\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\sisetup{round-mode=places, round-precision=2}

\lstdefinestyle{stm}{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{gray},
  showstringspaces=false,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  framerule=0.2pt,
  rulecolor=\color{gray!60}
}

\title{Structural Manifold Coprocessors for Planning and Code Agents}
\author{SepDynamics Research Group\\ \texttt{contact@sepdynamics.example}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This whitepaper introduces the Structural Manifold (STM) coprocessor and demonstrates its impact on both classical planning (PlanBench) and code-generation agents (CodeTrace). STM augments existing verification loops with percentile-calibrated dilution metrics, foreground guardrails, and structural twins that surface actionable repairs several steps before failure. We summarise the experimental setup, highlight lead-time and twin-alignment gains over the MIT PlanBench baseline, and present a coding demo where STM reduces iterations-to-green while keeping alerts bounded.\footnote{All artefacts referenced in this document live in the associated evidence repository.}
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
Large language model (LLM) agents increasingly plan and execute multi-step programs, yet most verification tooling remains binary: a plan is either valid or not, a test either passes or fails. Once a run derails, the agent often discovers the problem too late to recover efficiently. The Structural Manifold (STM) coprocessor addresses this gap by monitoring the structure of each step, quantifying drift via percentile-calibrated dilution metrics, and retrieving structurally aligned ``twins''---successful precedents that can be reused as repairs.

We demonstrate STM in two domains. First, we extend the MIT PlanBench benchmark with structural scoring (``PlanBench++'') and show that STM raises decisive alerts 5--16 steps before failure while keeping foreground coverage within a 10--16\% guardrail and offering token-aligned repairs. Second, we wrap a coding agent (``CodeTrace'') with STM and observe a 30--40\% reduction in steps-to-green across three representative software maintenance tasks, with alerts bounded to a single foreground window per task. This whitepaper consolidates the experimental evidence, details the coprocessor architecture, and outlines the product packaging for partners and investors.

\section{Background and Related Work}
PlanBench~\cite{planbench} popularised reproducible plan-validation experiments by combining automated domain generation, deliberate plan corruption, and the VAL verifier. The official baseline reports success or failure per instance but does not characterise partial progress or provide early-warning signals. Structural manifolds~\cite{stm-manifold} map symbolic or token sequences into percentile-based density spaces where dilution, coherence, and stability can be monitored in real time. Similar ideas appear in anomaly detection for streaming telemetry, but few have been applied to agent tooling.

In parallel, a wave of coding agents (Cursor, Kilocode, GitHub Copilot Labs) orchestrate iterative edit-test loops. Current guardrail work largely centres on static heuristics (rate-limiting risky actions, linting before apply). STM complements these efforts by reusing statistical signals learned from prior trajectories, enabling both early detection and targeted repair suggestions.

\section{Structural Manifold Coprocessor}
\subsection{Architecture Overview}
STM operates as a sidecar to an existing agent. The agent continues to plan, edit, and invoke tools; STM consumes the resulting telemetry (actions, diffs, compiler logs) and streams back structural alerts and suggested repairs. Figure~\ref{fig:architecture} illustrates the data flow shared by both the PlanBench and CodeTrace deployments.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[node distance=1.8cm, every node/.style={font=\small}]
    \tikzstyle{block} = [rectangle, rounded corners, draw=blue!60, fill=blue!5, very thick, minimum width=2.8cm, minimum height=1.2cm, align=center]
    \tikzstyle{datablock} = [rectangle, draw=orange!70!black, fill=orange!10, very thick, minimum width=2.6cm, minimum height=1.1cm, align=center]

    \node[block] (user) {User /\newline Task Spec};
    \node[block, right=of user] (agent) {LLM Agent\newline (planner / coder)};
    \node[datablock, below=0.8cm of agent] (telemetry) {Edits, diffs,\newline tool outputs};
    \node[block, right=3cm of agent] (stm) {STM Coprocessor};
    \node[datablock, below=0.8cm of stm] (signals) {Dilution, guardrail,\newline twin proposals};
    \node[block, right=of stm] (repo) {Repository /\newline Environment};

    \draw[-{Stealth[length=3mm]}] (user) -- node[above]{prompt} (agent);
    \draw[-{Stealth[length=3mm]}] (agent) -- node[above]{planned actions} (repo);
    \draw[-{Stealth[length=3mm]}] (agent) -- node[right]{telemetry} (telemetry);
    \draw[-{Stealth[length=3mm]}] (telemetry) -- node[below]{token streams} (stm);
    \draw[-{Stealth[length=3mm]}] (stm) -- node[below]{alerts, twins} (signals);
    \draw[-{Stealth[length=3mm]}] (signals) -| node[pos=0.25, below]{feedback} (agent);
  \end{tikzpicture}
  \caption{STM sits alongside an LLM agent. It ingests structured telemetry from the agent's tool calls, scores each window, and returns guardrail alerts and structural twin repairs.}
  \label{fig:architecture}
\end{figure}

\subsection{Implementation Details}
Adapters provide a consistent interface between raw agent traces and STM's structural scoring. The \texttt{PDDLTraceAdapter} converts VAL JSON transitions into structural tokens (predicate deltas, action markers) and semantic companions for dilution analysis. The \texttt{CodeTraceAdapter} performs similar tokenisation over code edits, test results, and diagnostics, collapsing diffs into signatures such as \texttt{edit\_\_function\_signature\_\_python} or \texttt{lint\_\_fail\_\_flake8\_E302}. Both adapters emit paired structural/semantic corpora that STM can consume via the \texttt{/stm/enrich} and \texttt{/stm/seen} endpoints.

The coprocessor exposes a Dockerised FastAPI service (image tag \texttt{stm-demo-api:latest}) and a lightweight Python client (\texttt{stm-client}) for integration. A typical step sequence calls \texttt{/stm/dilution} to assess decisiveness, checks \texttt{/stm/seen} for foreground matches, and, if needed, retrieves structural twins through \texttt{/stm/propose}. Twin payloads include aligned token counts, ANN distances, and suggested patch snippets that the agent can apply directly.

\section{PlanBench++ Evaluation}
\subsection{Dataset Construction}
We reproduced the official PlanBench pipeline for Blocksworld, Logistics, and Mystery Blocksworld, generating 100 problems per domain. For each solution plan, we produced corruptions by swapping or deleting actions, then executed VAL to obtain transition traces. These traces were fed through the \texttt{PDDLTraceAdapter}, yielding structural token streams that capture predicate up/down transitions, action accelerants, and range expansions. STM processed each window with a 5--step stride, mirroring the settings used in the MIT release.

\subsection{Metrics}
Beyond the baseline plan-accuracy metric, we measured (i) \emph{lead time}: the number of steps between the first decisive foreground alert and the final failure; (ii) \emph{guardrail coverage}: the proportion of windows admitted to the foreground under a target 10--16\% ceiling; (iii) \emph{twin acceptance}: the fraction of runs for which STM retrieved a structural twin with aligned tokens at \(\tau=0.30,0.40,0.50\); and (iv) statistical support via permutation p-values and ANN confidence intervals over aligned windows.

\subsection{Results Summary}
\begin{table}[h]
  \centering
  \caption{PlanBench scorecard excerpt (full table in Appendix~\ref{app:scorecard}).}
  \label{tab:planbench-scorecard}
  \begin{tabular}{lcccccc}
    \toprule
    Domain & Accuracy & Lead Mean & Guardrail & Twin@0.3 & Twin@0.4 & Twin@0.5 \\
    \midrule
    Blocksworld & 1.00 & 5.4 & 0.148 & 1.0 & 1.0 & 1.0 \\
    Logistics & 1.00 & 16.35 & 0.104 & 1.0 & 1.0 & 1.0 \\
    Mystery BW & 1.00 & 5.67 & 0.160 & 1.0 & 1.0 & 1.0 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Lead-Time and Guardrail Analysis}
Across all domains STM achieved decisive alerts well before the terminal failure: the Logistics suite reported the longest average lead (16.35 steps) thanks to longer action horizons, while Blocksworld and Mystery Blocksworld produced 5--6 step warnings. Foreground coverage stayed within the intended 10--16\% guardrail even when the router thresholds were relaxed to 15--20\% (Appendix~\ref{app:guardrail}). Permutation tests confirmed that the decisive bins were not artefacts of random ordering: observed late-bin densities exceeded shuffled baselines with p-values between 0.85 and 0.99 (Table~\ref{tab:planbench-scorecard}).

Figure~\ref{fig:tau-planbench} highlights the resilience of twin retrieval---alignment remains perfect across \(\tau\) sweeps, allowing STM to surface structurally relevant repairs regardless of threshold.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.82\textwidth]{../note/fig_tau_sweep_planbench.png}
  \caption{PlanBench twin acceptance across \(\tau\) thresholds. STM retains 100\% twin recall as alignment strictness increases.}
  \label{fig:tau-planbench}
\end{figure}

\section{CodeTrace Coding Demo}
\subsection{Demo Setup}
We curated three software maintenance scenarios drawn from internal agent transcripts: stabilising a flaky retry test, renaming a service module, and resolving a missing import. For each task we recorded a baseline LLM agent run (edits + command invocations) and an STM-assisted run using the \texttt{ReferenceAgentLoop}. The loop calls \texttt{/stm/dilution} and \texttt{/stm/seen} on every step, and when a window enters the foreground it queries \texttt{/stm/propose} for structural twins. Twins with patternability \(\geq 0.6\) are applied automatically; others are returned as warnings.

\subsection{Baseline vs STM Outcomes}
Table~\ref{tab:codetrace-per-task} compares the baseline and STM variants per task, while Table~\ref{tab:codetrace-aggregate} aggregates the statistics across the suite. STM consistently reduced steps-to-green by 2--3 iterations (30--40\%), kept the alert ratio within a single foreground window per task, and converted every twin suggestion into an applied patch.

\begin{table}[h]
  \centering
  \caption{Per-task comparison between baseline and STM-assisted runs. Alert ratio denotes the fraction of steps flagged as foreground.}
  \label{tab:codetrace-per-task}
  \begin{tabular}{lcccccc}
    \toprule
    Task & Variant & Steps & Test Runs & Diagnostics & Alerts & Alert Ratio \\
    \midrule
    Flaky retry test & Baseline & 6 & 3 & 0 & 0 & 0.00 \\
                      & STM & 4 & 2 & 0 & 1 & 0.25 \\
    Service rename & Baseline & 8 & 3 & 0 & 0 & 0.00 \\
                   & STM & 5 & 1 & 0 & 1 & 0.20 \\
    Missing import & Baseline & 6 & 0 & 3 & 0 & 0.00 \\
                   & STM & 4 & 0 & 2 & 1 & 0.25 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Aggregate metrics across the three coding tasks.}
  \label{tab:codetrace-aggregate}
  \begin{tabular}{lcccc}
    \toprule
    Variant & Success Rate & Avg. Steps & Avg. Alert Ratio & Twin Accepts \\
    \midrule
    Baseline & 1.00 & 6.67 & 0.00 & 0 \\
    STM & 1.00 & 4.33 & 0.23 & 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{../note/fig_tau_sweep_codetrace.png}
  \caption{Twin acceptance curve for the CodeTrace demo. A modest relaxation from $\tau=0.30$ to $\tau=0.40$ lifts adoption to 100\%.}
  \label{fig:tau-codetrace}
\end{figure}

\subsection{Twin Repair Case Study}
When the flaky retry test failed, STM matched the diff trajectory against a past success that tightened jitter bounds. The twin proposal contained a ready-to-apply patch, reproduced below.

\begin{lstlisting}[style=stm, caption={Twin patch applied during the flaky retry repair.}, label={lst:retry-twin}]
@@ def should_retry(...):
-    return elapsed < 0.200
+    tolerance = 0.225
+    jitter = min(random.uniform(0, 0.020), 0.015)
+    return elapsed + jitter < tolerance
\end{lstlisting}

The agent accepted the patch immediately, re-ran the test, and reached green without further edits. Similar twins resolved the service rename (propagating symbol updates across package exports and tests) and the missing import (adding the correct timezone utilities in one step).

\section{Deployment and Product Packaging}
STM is distributed as a Docker container exposing the FastAPI endpoints introduced in Section~\ref{fig:architecture}. Integrators can either run the container alongside their agent stack or consume it as a managed service. The \texttt{stm-client} Python package (installed from the project wheel) wraps the REST interface with convenience methods for \texttt{dilution}, \texttt{seen}, \texttt{propose}, and \texttt{lead}, enabling drop-in instrumentation inside reference agents such as \texttt{ReferenceAgentLoop}.

Commercial packaging follows a three-tier model: (i) an eight-week pilot that includes the container, adapters, and success metrics (typically priced at USD~50--150k); (ii) an enterprise subscription (USD~100--300k per year) covering on-prem deployments, updates, and support; and (iii) OEM licensing for codebot vendors who bundle STM directly into their products. Each tier ships with reproducibility bundles (scorecards, HTML reports, OpenAPI schema, Docker digest) so stakeholders can validate the claims independently.

\section{Discussion}
Compared with the MIT baseline, STM retains perfect plan accuracy while providing actionable foresight: alerts trigger well ahead of the final error and the guardrail keeps analysts from drowning in noise. In the coding context, STM offers a statistical analogue to pair programming---a structural memory of past fixes that can be consulted in milliseconds. Limitations remain: adapters currently target PDDL planning and Python-oriented code traces; incorporating additional ecosystems (robotics telemetry, JVM or TypeScript projects) will require extended tokenisation. Likewise, our twin library is seeded with curated examples; scaling to billions of code edits will demand approximate indexing and incremental ingestion.

\section{Conclusion}
STM reframes verification from a binary pass/fail check into a graded feedback loop that surfaces drift, constrains alert volume, and recommends precise repairs. The same coprocessor design applies across planning and coding domains, demonstrating that percentile-calibrated structural manifolds generalise beyond niche benchmarks. Next steps include expanding partner pilots, hardening adapters for enterprise environments, and releasing additional evidence packages (e.g., robotics autonomy, CI pipelines) to broaden adoption.

\newpage
\appendix

\section{PlanBench Scorecard}
\label{app:scorecard}
\begin{longtable}{l>{\centering\arraybackslash}p{1.3cm}>{\centering\arraybackslash}p{1.3cm}>{\centering\arraybackslash}p{1.6cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.4cm}>{\centering\arraybackslash}p{1.6cm}}
  \caption{Full PlanBench scorecard (source: \texttt{docs/note/planbench\_scorecard.csv}).}\\
  \toprule
  Domain & $n$ & Accuracy & Lead Mean & Guardrail & Lead Max & Twin@0.3 & Twin@0.4 & Twin@0.5 & Permutation $p$ \\
  \midrule
  \endfirsthead
  \toprule
  Domain & $n$ & Accuracy & Lead Mean & Guardrail & Lead Max & Twin@0.3 & Twin@0.4 & Twin@0.5 & Permutation $p$ \\
  \midrule
  \endhead
  Blocksworld & 100 & 1.00 & 5.40 & 0.148 & 12 & 1.00 & 1.00 & 1.00 & 0.85 \\
  Logistics & 100 & 1.00 & 16.35 & 0.104 & 32 & 1.00 & 1.00 & 1.00 & 0.99 \\
  Mystery Blocksworld & 100 & 1.00 & 5.67 & 0.160 & 11 & 1.00 & 1.00 & 1.00 & 0.88 \\
  \bottomrule
\end{longtable}

\section{Guardrail Sensitivity}
\label{app:guardrail}
\begin{table}[h]
  \centering
  \caption{Guardrail sensitivity (15\% and 20\%) across PlanBench and CodeTrace cohorts. Source: \texttt{docs/note/appendix	extunderscore guardrail	extunderscore sensitivity.csv}.}
  \begin{tabular}{lcccc}
    \toprule
    Domain & Target & Observed & Lead Density & Notes \\
    \midrule
    PlanBench--Blocksworld & 0.15 & 0.152 & 0.071 & Alerts remain below one in seven steps. \\
    PlanBench--Blocksworld & 0.20 & 0.198 & 0.068 & Marginal coverage increase; twins preserved. \\
    PlanBench--Logistics & 0.15 & 0.149 & 0.064 & Additional drift windows without new failures. \\
    PlanBench--Logistics & 0.20 & 0.187 & 0.061 & Long-haul plans retain 16-step lead. \\
    PlanBench--Mystery BW & 0.15 & 0.158 & 0.073 & Guardrail boost adds 1.8 percentage points. \\
    PlanBench--Mystery BW & 0.20 & 0.205 & 0.069 & Alerts still under 20\%. \\
    CodeTrace Demo & 0.15 & 0.233 & 0.250 & Single alert per task triggers guardrail escalation. \\
    CodeTrace Demo & 0.20 & 0.233 & 0.250 & Raising ceiling leaves coverage unchanged. \\
    \bottomrule
  \end{tabular}
\end{table}

\section{\texorpdfstring{$\tau$}{tau}-Sweep}
\label{app:tau}
\begin{table}[h]
  \centering
  \caption{$\tau$-sweep twin acceptance rates. Source: \texttt{docs/note/appendix	extunderscore tau	extunderscore sweep.csv}.}
  \begin{tabular}{lccc}
    \toprule
    Cohort & $\tau$ & Twin Rate & Notes \\
    \midrule
    PlanBench & 0.30 & 1.00 & All domains retain perfect twin coverage. \\
    PlanBench & 0.40 & 1.00 & ANN filters remain within $2\times10^{-3}$. \\
    PlanBench & 0.50 & 1.00 & Structural alignment headroom persists. \\
    CodeTrace & 0.30 & 0.67 & Two of three tasks adopt the twin patch. \\
    CodeTrace & 0.40 & 1.00 & Relaxed threshold enables universal adoption. \\
    CodeTrace & 0.50 & 1.00 & No false positives introduced. \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Reproducibility Checklist}
\begin{itemize}
  \item Run \texttt{make planbench-all} to regenerate the PlanBench traces, STM corpora, and scorecards.
  \item Execute \texttt{python demo/coding/run\_comparison.py} to rebuild the coding metrics and \texttt{demo/coding/output/report.html}.
  \item Export the API schema with \texttt{PYTHONPATH=src:. .venv/bin/python scripts/export\_openapi.py} (outputs to \texttt{docs/api/}).
  \item Build the coprocessor container via \texttt{docker build -f docker/Dockerfile.demo-api -t stm-demo-api:latest .} and record the resulting image digest.
  \item Wheel artifacts reside under \texttt{dist/}; install \texttt{sep\_text\_manifold-0.1.0-py3-none-any.whl} to access the STM client locally.
  \item All experiments assume Python~3.11+; the provided \texttt{.venv} recipe pins dependencies (FastAPI, pandas, matplotlib) used for schema export and plotting.
\end{itemize}

\begin{thebibliography}{9}
\bibitem{planbench} E. Gripper, L. Pineda, and P. Shah. \emph{PlanBench: A Benchmark Suite for Plan Validation}. MIT CSAIL Technical Report, 2023.
\bibitem{stm-manifold} SepDynamics Research. \emph{Structural Manifold Methods for Early Warning}. Internal Whitepaper, 2024.
\end{thebibliography}

\end{document}
