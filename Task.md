# Transformer-Guided Reality Filter Roadmap

This note distils insights from *Attention Is All You Need* (Vaswani et al., 2017) into concrete refinements for the Reality Filter stack. Citations below reference the original paper’s figures/sections so we can trace design choices back to first principles.

## 1. Conceptual Alignment
- **Content-addressable routing.** Scaled dot-product attention (Sec. 3.2.1, Fig. 2) formalises the “reach across the manifold” behaviour we approximate with margin lookups. Tokens attend globally by similarity instead of proximity; we should treat our reliability map as a learned attention kernel rather than hand-coded heuristics.
- **Multi-head diversity.** Multi-head attention (Sec. 3.2.2) mirrors our need for parallel structural/semantic/hazard filters. Assign specialised heads to support discovery, contradiction spotting, recurrence, and noise suppression instead of separate bespoke passes.
- **Short dependency paths.** Table 1 shows attention achieving O(1) maximum path length, which is exactly the property we rely on for rupture-based recalls. This justifies using attention instead of deeper conv/RNN stacks for long-span evidence retrieval.
- **Phase-aware encodings.** Sinusoidal position encodings (Sec. 3.5) already offer extrapolatable phase signals. We can extend them with our prime/beat encodings as extra channels while keeping compatibility with the base design.
- **Cross-attention for evidence gating.** Encoder–decoder attention (Sec. 3.2.3) is a drop-in analogue for “query claims against evidence memory.” Our current manual URI gating can be replaced with a cross-attention block over the truth-pack state.

## 2. Minimal O-Space Transformer Blueprint
1. **Backbone.** A lightweight Transformer encoder over the stream (2–4 layers, 8 heads) with causal masking, plus cross-attention into a key/value cache of indexed evidence windows.
2. **Reliability head.** Feed the final hidden state into an MLP producing (a) admit probability and (b) support margin. Train on the final adjudicated labels (post-repair) so hallucination rate reflects the end decision.
3. **Phase channels.** Concatenate standard sinusoidal embeddings with our existing phase/prime ticks. Keep them additive so we can ablate each component cleanly.
4. **Structured sparsity.** Adopt top-k attention or sliding-window attention for bulk spans, but reserve a handful of global “rupture tokens” that can attend everywhere (matching Sec. 4’s restricted attention discussion).

## 3. Integration Steps
- **Model skeleton (`src/sep_text_manifold/attn_ospace.py`).** Implement the backbone + reliability head. Export a forward signature that accepts span tokens and evidence memory tensors.
- **Training harness (`scripts/train_reliability_attn.py`).** Build a PyTorch Lightning (or vanilla PyTorch) loop that ingests the existing eval JSONL, uses the gold `final_answer`/`supported` labels, and logs admit precision/recall.
- **Service swap (`scripts/reality_filter_eval.py`).** Replace the current token-support override with the model’s reliability head output. Threshold on calibrated probabilities rather than raw margin comparisons.
- **Artifact logging.** Persist attention maps per head under `results/eval/<pack>/attention_heads/` so the report can display which evidence the model actually used (cf. paper’s qualitative plots on pp. 13–15).
- **CI guardrails.** Add a regression test that compares confusion matrices generated by the model-backed evaluator with those in the summary JSON; fail the run if they diverge.

## 4. Experiments Worth Running
- **E1 – Final-answer scoring.** Train/evaluate on the repaired answers only. Metric: macro-F1, admit precision/recall, calibration error.
- **E2 – Token-support calibration.** Enforce dual thresholds: attention mass on evidence head ≥ τ and admit probability ≥ σ. Plot reliability diagrams to ensure low-margin URI hits no longer slip through.
- **E3 – Phase encoding ablation.** Compare vanilla sinusoids vs. sinusoids+phase channel; track long-lag recurrence recall and head specialisation.
- **E4 – Attention specialisation.** Probe heads for support/contradiction/noise functions and surface them in reports (attention entropy, token attribution).
- **E5 – Sparsity sweep.** Vary local window size and number of global rupture tokens, ensuring throughput targets (≥1k rps) hold without degrading admit precision.

## 5. Acceptance Criteria
- Admit precision/recall gains with hallucination rate reflecting final decisions (no more “perfect confusion + 100% hallucination”).
- Evidence heads attend to real citations; reports include attention overlays for transparency.
- Throughput remains within current SLOs using restricted attention + rupture tokens.
- CI enforces detail-summary parity and rejects regressions in confusion matrices or calibration metrics.

## 6. Open Questions
- Best way to initialise the evidence memory (precomputed embeddings vs. learned KV cache)?
- How aggressively can we prune attention without hurting rare long-range supports?
- Should we expose the reliability head as a standalone API so other internal services can query admit probabilities directly?

Once we agree on this blueprint, we can spin up the model module and training loop in parallel with the existing evaluation pipeline, then flip the switch when metrics and attention audits look solid.
