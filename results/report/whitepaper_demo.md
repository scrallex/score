# Reality Filter Report — whitepaper_demo

## KPIs

- **approved**: 3
- **blocked**: 2
- **repaired**: 2
- **hallucination_rate**: 0.4
- **repair_yield**: 0.5
- **citation_coverage**: 1.0
- **latency_ms_p50**: 93.31
- **latency_ms_p90**: 99.944
- **latency_ms_budget**: 120.0
- **latency_budget_breach_rate**: 0.0

## Permutation Test

Observed coverage 0.000 with p-value 1.0000.

## Threshold Sweep (top coverage)

| r_min | lambda_max | sigma_min | coverage | admitted | total |
| --- | --- | --- | --- | --- | --- |
| 1 | 0.12 | 0.15 | 0.000 | 0 | 5 |

Full CSV: results/sweeps/whitepaper_demo.csv

## Evaluation Metrics

- **macro_f1**: 1.0
- **baseline_macro_f1**: 0.6049382716049383
- **macro_f1_delta**: 0.3950617283950617
- **dev_macro_f1**: 1.0
- **sanity_flags_count**: 0
- **metrics.total**: 24
- **metrics.hallucination_rate**: 1.0
- **metrics.repair_yield**: 0.7916666666666666
- **metrics.citation_coverage**: 0.4583333333333333
- **best_thresholds**:
  - r_min: 1
  - lambda_max: 0.55
  - sigma_min: 0.15
  - structural_threshold: 0.46
  - semantic_threshold: 0.25

## Before vs After Examples

### WHITU037 (UNVERIFIABLE → UNVERIFIABLE)

**Question:** What policy covers 'unverified_claim_037'?

**Baseline:** I cannot find any information about unverified_claim_037.

**Filtered:** No supporting evidence.

---
### WHITS031 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'lambda'?

**Baseline:** The documentation states that lambda is covered in detail.

**Filtered:** lambda (see doc://whitepaper#lambda)

---
### WHITS047 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'telemetry'?

**Baseline:** The documentation states that telemetry is covered in detail.

**Filtered:** telemetry (see doc://whitepaper#telemetry)

---
### WHITU033 (UNVERIFIABLE → UNVERIFIABLE)

**Question:** What policy covers 'unverified_claim_033'?

**Baseline:** I cannot find any information about unverified_claim_033.

**Filtered:** No supporting evidence.

---
### WHITS007 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'twin'?

**Baseline:** The documentation states that twin is covered in detail.

**Filtered:** twin (see doc://whitepaper#twin)

---
### WHITR009 (REFUTED → REFUTED)

**Question:** Is it correct that 'filter' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies filter.

**Filtered:** filter (see doc://whitepaper#filter)

---
### WHITS030 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'from'?

**Baseline:** The documentation states that from is covered in detail.

**Filtered:** from (see doc://whitepaper#from)

---
### WHITS032 (SUPPORTED → SUPPORTED)

**Question:** What does the documentation state about 'logistics'?

**Baseline:** The documentation states that logistics is covered in detail.

**Filtered:** logistics (see doc://whitepaper#logistics)

---
### WHITR012 (REFUTED → REFUTED)

**Question:** Is it correct that 'instruct' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies instruct.

**Filtered:** instruct (see doc://whitepaper#instruct)

---
### WHITR005 (REFUTED → REFUTED)

**Question:** Is it correct that 'before' is explicitly denied in the pack?

**Baseline:** The documentation explicitly denies before.

**Filtered:** before (see doc://whitepaper#before)

---
---
Generated by scripts/reality_filter_report.py

## Reliability Experiments (2025-10-05)

### FEVER retraining

- GPU run with the attention-backed reliability head (epochs=3, batch=64) reached **val F1 0.715** and **test F1 0.756** with admit threshold 0.5 / margin 0.8.
- Calibration sweep selected the same thresholds (F1 0.717). Metrics logged in `results/experiments/fever_reliability_runs.json` and checkpoint stored at `models/reliability_fever_attn_full.pt`.

### Ablations

| configuration | val F1 | test F1 | notes |
| --- | --- | --- | --- |
| no cross-attention | 0.014 | 0.050 | collapses without evidence cross-attention |
| no phase channel | 0.585 | 0.690 | moderate drop versus full model |
| feature dim = 16 | 0.665 | 0.710 | padding structural features helped calibration |
| MLP baseline | 0.667 | 0.712 | simple feed-forward over averaged metrics |

Full metrics and checkpoints captured in `results/experiments/fever_reliability_runs.json`.

### SciFact transfer

- Evaluated the FEVER-trained model on SciFact (809 train, 150 val, 150 test) using `scripts/convert_scifact_to_eval.py`.
- Model currently predicts "reject" for all claims (**val/test F1 = 0.0**, Brier 0.44 / 0.39). Heatmaps for 50 examples saved in `results/eval/scifact_attention/`.
- SciFact metrics recorded in `results/experiments/scifact_generalisation.json`.

### Attention visualisations

- FEVER validation/test attention plots saved to `results/eval/fever_attention/`.
- SciFact attention plots saved to `results/eval/scifact_attention/` (limit 50 per split).

### Next steps

1. Wire HoVer ingestion (`scripts/convert_hover_to_eval.py`) so the Transformer can be evaluated on multi-hop claims. The release requires sentence-level lookup from `wiki_wo_links.db`.
2. Track why SciFact transfer collapses—consider fine-tuning on SciFact or adding domain adaptation.
3. Fold the new metrics and attention plots into the whitepaper experiment section.
