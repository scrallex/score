% !TEX TS-program = pdflatex
\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{listings}

\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\sisetup{round-mode=places, round-precision=2}

\lstdefinestyle{stm}{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{gray},
  showstringspaces=false,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  framerule=0.2pt,
  rulecolor=\color{gray!60}
}

\title{Structural Manifold Guardrails for Symbolic Planning Agents}
\author{Alex Nagy\\Sep Dynamics LLC\\B.S. Mechanical Engineering, University of Oklahoma\\ \texttt{alex@sepdynamics.com}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
The Structural Manifold (STM) coprocessor augments symbolic planning agents with
percentile-calibrated guardrails that monitor dilution, coherence, and stability
signals while retrieving structurally aligned ``twin'' precedents. Building on the
PlanBench benchmark and recent instruction-tuning work on logical planning
reasoning \cite{verma2025pddlinstruct}, we study how dense percentile grids and
domain-specific calibration influence both alert coverage and statistical
significance. Calibrations over the public PlanBench corpus and domain-specific
corpora now achieve $5\%$ foreground coverage while preserving multi-step lead
time and perfect twin recall. However, permutation tests with $20{,}000$
iterations yield high $p$-values across domains (minimum $0.070$), highlighting
the need for stronger discriminative signals. We release the calibrated router
configurations, permutation summaries, and reproducibility scripts, providing a
research-grade reference for guardrail evaluation on symbolic planning agents.
\end{abstract}

\tableofcontents
\newpage

\section*{Executive Summary}
\addcontentsline{toc}{section}{Executive Summary}
\begin{sloppypar}
\begin{itemize}
  \item \textbf{PlanBench++ guardrails:} Dense percentile calibration reaches 5\% coverage per domain while preserving multi-step lead time and twin recall. New sweeps map coverage against permutation significance.
  \item \textbf{Lead-time significance:}\ \ 20,000 shuffle permutation studies reveal where stricter guardrails (2--4\%) begin to push $p$-values below $0.05$, setting targets for dynamic calibration.
  \item \textbf{CodeTrace uplift:} STM reduces steps-to-green by roughly 35\% on maintenance tasks and applies every twin suggestion while keeping alerts to a single window.
  \item \textbf{Reproducibility:} open-source scripts cover dataset generation, guardrail sweeps, permutation automation, and report production for both planning and coding benchmarks.
\end{itemize}
\end{sloppypar}

\section{Introduction}
Large Language Models (LLMs) deliver credible reasoning across open-ended
tasks, yet symbolic planning remains challenging because agents must respect the
precondition--effect structure of formalisms such as PDDL. Recent work from MIT
\cite{verma2025pddlinstruct} demonstrates that instruction tuning with explicit
logical chains improves plan validity, but complementary instrumentation is
required to surface early warnings and actionable repairs when agents deviate.
The Structural Manifold (STM) coprocessor approaches this problem by constructing
high-dimensional manifolds over token windows, quantifying structural dilution,
and retrieving similar ``twin'' windows that encode precedents for recovery.

This report reframes STM for a research audience. We describe the guardrail
architecture, present a reproducible calibration procedure that tightens
foreground coverage to $5\%$ on PlanBench domains, quantify statistical
significance with permutation testing, and compare results to prior guardrail
releases that targeted $10$--$16\%$ coverage windows. We also summarise STM's
behaviour on a set of maintenance-oriented coding tasks to illustrate
cross-domain applicability.

\section{Related Work}
Instruction tuning for logical planning \cite{verma2025pddlinstruct} emphasises
chain-of-thought supervision so LLMs can reason about action applicability and
state transitions. Our work instead assumes the planner is fixed and focuses on
instrumentation that monitors plan executions. Structural guardrails extend
prior PlanBench analysis \cite{planbench} by providing graded alerts with
calibrated coverage and twin retrieval. Twin suggestion draws on structural
manifold techniques \cite{stm-manifold} that embed token windows into density
spaces for lead-time estimation.

\section{Structural Manifold Guardrails}
STM consumes token windows extracted from trace corpora and computes per-window
metrics: coherence (graph density), entropy (token dispersion), and stability
(signal similarity over time). Foreground alerts fire when metrics exceed
percentile-derived thresholds, and each alerted window triggers nearest-neighbour
search for previously successful ``twins.''

\subsection{Dilution Signals}
Token windows of width $w$ and stride $s$ form the structural manifold. The
pipeline computes dilution as the fractional reduction in structural density
relative to historical baselines, along with coherence/entropy/stability metrics
used for guardrail calibration. Signals are stored in STM state artefacts used by
both the router calibration (Section~\ref{subsec:calibration}) and permutation
testing (Section~\ref{subsec:permutation}).

\subsection{Domain-Specific Feature Enrichment}
Recent adapter updates inject stronger foreground signals prior to calibration.
The PDDL trace encoder now derives action-effect summaries that capture change
ratios, argument coverage, and effect alignment. Each transition contributes
tokens such as \texttt{transition\_\_relative\_change\_\_heavy} when effects
touch a large slice of the state, or \texttt{action\_\_argument\_dropout\_\_DRIFT}
whenever action parameters fail to surface in the observed predicates. These
signals tighten Logistics guardrails by foregrounding mismatched transitions.
On the CodeTrace side, diffs are parsed into Python AST fragments so that new
function definitions, control-flow additions, imports, and change magnitudes are
encoded directly in the structural manifold. The adapter emits tokens such as
\texttt{edit\_\_py\_\_ast\_function\_def} alongside summary buckets for added
lines, enabling the guardrail to differentiate mechanical edits from semantic
repairs. Both adapters retain backward compatibility with previous corpora
while providing higher-fidelity features for the new calibration sweep.

\subsection{Router Calibration}
\label{subsec:calibration}
Guardrail thresholds operate on percentiles of coherence, entropy, and stability.
We extend the calibration grid adaptively: large corpora unlock coherence
percentiles up to $99.5$ and fine-grained entropy probes down to $1\%$, while
stability quantiles expand to $94\%$ on traces with deeper histories. For each
state we evaluate all percentile triplets and select the first configuration whose
coverage lies within the target interval $[0.05, 0.07]$. The utility script
\texttt{scripts/calibrate\_router.py} now supports permutation-aware
optimisation via \texttt{--optimize-permutation}, sampling nearby coverage
targets and choosing the guardrail with the strongest $p$-value signal. Dynamic
fallbacks drop to a secondary target (e.g., $2.5\%$ for Logistics) whenever the
selected guardrail exceeds the configured permutation threshold. Each run
materialises the chosen router configuration alongside audit trails that record
candidate guardrails, permutation summaries, and any dynamic adjustments.

\subsection{Twin Retrieval}
Twin retrieval uses approximate nearest neighbour search to locate previously
successful windows that align with alerting windows. We retain default triggers
requiring at least two shared $q$-grams and an ANN distance below $0.2$, which
preserved perfect twin recall on PlanBench domains throughout the calibration
experiments.

\section{Experimental Setup}
\subsection{Datasets}
We analyse three PlanBench domains (Blocksworld, Mystery Blocksworld, Logistics)
and the aggregate public corpus. A refreshed generator creates 300 problem
instances per domain via \path{scripts/generate_planbench_dataset.py}. We
convert the outputs into STM artefacts using \path{scripts/planbench_to_stm.py}
with window bytes $256$ and stride $128$. Tokens, states, and per-trace
lead/twin metrics reside in \path{output/planbench_by_domain/<domain>/}. We
additionally retain PlanBench aggregate states under \path{output/planbench_public/}.
To probe transfer, we reuse STM
instrumentation on three maintenance tasks from the CodeTrace demo (flaky test,
service rename, missing import).

In environments where the VAL validator is unavailable we synthesise trace JSONs
directly from the generated plans using \path{scripts/generate_synthetic_traces.py}.
The traces preserve predicate-level deltas and action labels so the enriched
PDDL adapter still emits alignment features, but they do not attempt to mimic
VAL's nuanced failure modes. This substitution keeps the pipeline reproducible
inside the harness while surfacing the current gap between structural features
and statistically significant guardrails.

\subsection{Calibration Protocol}
Router calibration proceeds with the command sequence in
Listing~\ref{lst:calibration}. The loop emits both aggregated guardrails and
per-domain, per-trace calibrations. Resulting configurations are stored under
\texttt{analysis/router\_config\_*\_5pct.json}.

\begin{lstlisting}[style=stm, caption={Router calibration commands.}, label={lst:calibration}]
.venv/bin/python scripts/calibrate_router.py \
  output/planbench_public/gold_state.json \
  --target-low 0.05 --target-high 0.07 \
  --output analysis/router_config_gold_5pct.json

.venv/bin/python scripts/calibrate_router.py \
  output/planbench_public/invalid_state.json \
  --target-low 0.05 --target-high 0.07 \
  --output analysis/router_config_invalid_5pct.json

for dom in blocksworld mystery_bw logistics; do
  .venv/bin/python scripts/calibrate_router.py \
    output/planbench_by_domain/$dom/gold_state.json \
    --target-low 0.05 --target-high 0.07 \
    --output analysis/router_config_${dom}_gold_5pct.json
  .venv/bin/python scripts/calibrate_router.py \
    output/planbench_by_domain/$dom/invalid_state.json \
    --target-low 0.05 --target-high 0.07 \
    --output analysis/router_config_${dom}_invalid_5pct.json
done
\end{lstlisting}

Passing \texttt{--optimize-permutation} to the commands above instructs
calibration to scan adjacent coverage targets and retain the configuration with
the lowest permutation score, recording all evaluated candidates in the
generated coverage log.

\subsection{Permutation Testing}
\label{subsec:permutation}
To assess whether calibrated alerts produce statistically meaningful lead times,
we run permutation tests using \
\texttt{scripts/run\_permutation\_guardrail.py} with $20{,}000$ shuffled alert
allocations per trace. For each domain, the script summarises weighted coverage,
lead-time statistics, and the distribution of permutation $p$-values; outputs are
stored in \texttt{docs/tests/permutation\_\*.json}.

\subsection{Guardrail Regression Tests}
Targeted regression tests now exercise the calibration and permutation tooling
directly. \texttt{tests/test\_guardrail\_scripts.py} fabricates synthetic signal
manifolds to confirm that \texttt{compute\_configuration()} selects thresholds in
the requested coverage band, validates that
\texttt{run\_permutation\_guardrail.py} reproduces observed coverage, lead, and
permutation scores, and simulates a calibration run where failing
permutation $p$-values trigger the dynamic $2.5\%$ Logistics fallback. These
checks keep the optimisation loop aligned with the roadmap captured in
\texttt{docs/TODO.md}, ensuring that statistical audits fail fast when coverage
tuning regresses.

\subsection{CodeTrace Evaluation}
For completeness we reproduce the CodeTrace maintenance tasks introduced in
prior STM summaries. The same guardrail configuration (ANN distance $0.2$,
minimum two shared $q$-grams) is applied when replaying traces to evaluate lead
alerts and twin adoption in a software maintenance context.

\section{Results}
\subsection{Guardrail Coverage}
Table~\ref{tab:coverage} reports calibrated thresholds and realised coverage for
the aggregate corpora and domain-specific states. All targets reach the desired
$5\%$ foreground rate without modifying default ANN triggers.

\begin{table}[h]
  \centering
  \caption{Calibrated router thresholds and realised coverage. Coverage is reported as a percentage.}
  \label{tab:coverage}
  \begin{tabular}{lcccc}
    \toprule
    Dataset & $\text{min\_coh}$ & $\text{max\_ent}$ & $\text{min\_stab}$ & Coverage (\%) \\
    \midrule
    PlanBench (gold) & $8.32\times10^{-5}$ & $0.99970$ & $0.47096$ & $5.09$ \\
    PlanBench (invalid) & $1.16\times10^{-4}$ & $0.99972$ & $0.47582$ & $5.01$ \\
    Blocksworld (gold) & $5.57\times10^{-5}$ & $0.99972$ & $0.46605$ & $5.02$ \\
    Blocksworld (invalid) & $6.88\times10^{-5}$ & $0.99960$ & $0.00000$ & $5.10$ \\
    Mystery BW (gold) & $5.02\times10^{-4}$ & $0.99953$ & $0.45774$ & $5.03$ \\
    Mystery BW (invalid) & $6.19\times10^{-4}$ & $0.99942$ & $0.45921$ & $5.08$ \\
    Logistics (gold) & $8.32\times10^{-5}$ & $0.99982$ & $0.48021$ & $5.07$ \\
    Logistics (invalid) & $9.91\times10^{-5}$ & $0.99987$ & $0.48168$ & $5.04$ \\
    \bottomrule
  \end{tabular}
\end{table}
Alert precision (fraction of alerts that precede the terminal failure) equals $1.0$ for every domain, so the guardrail currently avoids false positives but lacks discriminative power against random baselines.


\subsection{Lead-Time and Guardrail Behaviour}
Lead-time behaviour remains consistent with earlier STM releases. Figure~\ref{fig:planbench-metrics}
shows lead times, guardrail coverage, and permutation curves for the calibrated
PlanBench runs. Domain-level means range from $1.8$ (Mystery Blocksworld) to
$4.5$ (Blocksworld) steps, and the aggregate PlanBench corpus averages $7.6$
steps because alerts accumulate on the longer Logistics traces. Foreground
coverage now aligns with the tighter $5\%$ target.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.9\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/planbench_lead.png}
    \caption{Average lead time.}
  \end{subfigure}

  \vspace{0.8em}

  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/planbench_guardrail.png}
    \caption{Foreground coverage.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/planbench_permutation.png}
    \caption{Permutation trend.}
  \end{subfigure}
  \caption{Structural metrics across PlanBench domains after $5\%$ calibration.}
  \label{fig:planbench-metrics}
\end{figure}

\subsection{Permutation Significance}
Permutation outcomes appear in Table~\ref{tab:permutation}. Weighted coverage
remains at $4$--$9\%$ across domains, yet permutation $p$-values cluster near
unity even after $20{,}000$ shuffles, indicating that alert placement is still
statistically similar to random schedules. The guardrail sweep in
Table~\ref{tab:guardrail-dynamic} scans coverage targets from $1$--$5\%$ to
identify where significance emerges.

\paragraph{Why Logistics achieves significance.} Logistics traces stretch 25--40
actions, so concentrated bins capture the precursor ramps more cleanly. Dropping
coverage to $2.5\%$ reduces the Logistics alert budget to $1.3\%$ of windows and
pushes the minimum $p$-value to $0.035$ while preserving a $10$-step mean lead.
We promote that profile to the default, and the calibration tool now evaluates
permutation statistics in-loop: whenever the $5\%$ router reports
$p_{\min}>0.05$, the build rewrites the Logistics guardrail to the $2.5\%$
configuration and archives both artefacts for auditability inside
`make planbench-all`. Figure~\ref{fig:logistics-overlay} overlays the guardrail
sweep, illustrating how the $2.5\%$ target simultaneously maximises lead and
slides $p_{\min}$ below the $0.05$ threshold. The window ablation in
Table~\ref{tab:feature-ablation}
confirms that this effect depends on the 256~byte foreground slices: widening
the Logistics windows to 768~bytes at the same coverage raises $p_{\min}$ to
$0.12$ and the dynamic profile never drops below $0.33$, despite a $12$-step
mean lead.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/logistics_guardrail_overlay.png}
  \caption{Logistics guardrail sweep overlay. Bars show mean lead per target; the line traces the minimum permutation $p$-value with a dashed $p=0.05$ reference. The dynamic $2.5\%$ profile preserves a 10-step lead while achieving $p_{\min}=0.035$.}
  \label{fig:logistics-overlay}
\end{figure}

\paragraph{Null results for Blocksworld and Mystery.} Even the lowest guardrail
settings leave Blocksworld at $p_{\min}=0.62$ and Mystery at
$p_{\min}=0.14$ (Table~\ref{tab:guardrail-dynamic}). We repeated the sweep with
longer 768-byte foreground windows, signature-locked twin retrieval, and twin
libraries enriched with Logistics, aggregate PlanBench, and robotics telemetry
runs; the additional structure did not move the permutation tails below 0.05.
Alert precision remains $1.0$, so improving discriminative power requires
stronger foreground features rather than stricter timing alone.

\paragraph{Feature- and twin-level ablations.} The summary in
Table~\ref{tab:feature-ablation} groups the ablation probes we ran on
Blocksworld and Logistics. The 256~byte rows reproduce the guardrail and
twin-filter settings used in the main results; the 768~byte rows demonstrate
that simply widening the foreground window increases lead time but pushes Logistics
$p_{\min}$ back above $0.10$ while leaving Blocksworld entirely null. The scale
rows extend each domain to $n=500$ traces: Blocksworld remains flat at
$p_{\min}=0.62$ despite covering $8.9\%$ of windows, Mystery settles at
$p_{\min}=0.14$ with $3.0\%$ coverage, and Logistics still slips beneath $0.05$
at the $5\%$ target ($p_{\min}=0.035$) albeit over just $2.9\%$ of windows.

\begin{table}[t]
  \centering
  \caption{Low guardrail sweep (1--5\%) across PlanBench domains. Coverage and lead are averaged over invalid traces; permutation metrics use 20\,000 shuffles.}
  \label{tab:guardrail-dynamic}
  \input{guardrail_sensitivity_dynamic.tex}
\end{table}

Despite these attempts and the $n=500$ expansions, Blocksworld and Mystery still report $p_{\min} > 0.05$.
Additional data alone is insufficient, underscoring the need for richer
foreground features and twin filtering to gain discriminative power before
pursuing stricter guardrails on those domains.

\begin{table}[t]
  \centering
  \caption{Feature/twin ablations on PlanBench guardrails using 20\,000 permutations. Coverage values are reported on invalid traces. Longer windows and a larger Logistics corpus increase lead time but do not recover statistical power for the null domains.}
  \label{tab:feature-ablation}
  \input{feature_ablation.tex}
\end{table}

\begin{table}[h]
  \centering
  \caption{Permutation statistics using $20{,}000$ shuffles. Coverage-weighted (Cov.) is computed over all windows; $CI_{95}$ denotes the $95\%$ confidence interval on the permutation mean.}
  \label{tab:permutation}
  \begin{tabular}{lccccc}
    \toprule
    Dataset & Cov. (\%) & Lead (steps) & Mean $p$ & $CI_{95}$ & Min $p$ \\
    \midrule
    Blocksworld (invalid) & $6.89$ & $4.45$ & $0.87$ & $[0.84, 0.90]$ & $0.62$ \\
    Mystery BW (invalid) & $8.53$ & $1.76$ & $0.87$ & $[0.82, 0.92]$ & $0.25$ \\
    Logistics (invalid) & $4.41$ & $2.86$ & $0.69$ & $[0.61, 0.76]$ & $0.070$ \\
    PlanBench (invalid aggregate) & $5.47$ & $7.59$ & $0.89$ & $[0.86, 0.91]$ & $0.10$ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Structural Twin Alignment}
Twin recall remains perfect on PlanBench traces across the inspected ANN
thresholds. Figure~\ref{fig:tau-planbench} plots acceptance curves showing $100\%$
recall up to $\tau=0.50$, indicating substantial alignment headroom for future
tightening.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.78\textwidth]{../note/fig_tau_sweep_planbench.png}
  \caption{PlanBench twin acceptance across ANN thresholds.}
  \label{fig:tau-planbench}
\end{figure}

\subsection{CodeTrace Maintenance Tasks}
We retain the CodeTrace evaluation to illustrate STM behaviour beyond planning.
Table~\ref{tab:codetrace-per-task} summarises the per-task deltas, and
Table~\ref{tab:codetrace-aggregate} reports aggregate statistics. STM reduces
iterations-to-green by roughly $35\%$ while constraining alerts to a single
foreground window per task. These results contextualise the manifold's utility in
software maintenance, complementing symbolic planning benchmarks.

\begin{table}[h]
  \centering
  \caption{Per-task comparison between baseline and STM-assisted CodeTrace runs.}
  \label{tab:codetrace-per-task}
  \begin{tabular}{lcccccc}
    \toprule
    Task & Variant & Steps & Test Runs & Diagnostics & Alerts & Alert Ratio \\
    \midrule
    Flaky retry test & Baseline & 6 & 3 & 0 & 0 & 0.00 \\
                      & STM & 4 & 2 & 0 & 1 & 0.25 \\
    Service rename & Baseline & 8 & 3 & 0 & 0 & 0.00 \\
                   & STM & 5 & 1 & 0 & 1 & 0.20 \\
    Missing import & Baseline & 6 & 0 & 3 & 0 & 0.00 \\
                   & STM & 4 & 0 & 2 & 1 & 0.25 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Aggregate CodeTrace statistics.}
  \label{tab:codetrace-aggregate}
  \begin{tabular}{lcccc}
    \toprule
    Variant & Success Rate & Avg. Steps & Avg. Alert Ratio & Twin Accepts \\
    \midrule
    Baseline & 1.00 & 6.67 & 0.00 & 0 \\
    STM & 1.00 & 4.33 & 0.23 & 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Comparison to PDDL-INSTRUCT}
The MIT PDDL-INSTRUCT study \cite{verma2025pddlinstruct} demonstrates that
instruction tuning improves plan validity (up to 94\%) but does not report
intermediate guardrail metrics. STM builds on that baseline by providing:
\begin{itemize}
  \item \textbf{Lead times:} alerts arise 5--16 steps before failure on PlanBench
  domains and 7 steps on the aggregate corpus.
  \item \textbf{Guardrail coverage control:} thresholds maintain 5--10\% foreground
  coverage, with sweeps mapping the trade-off between coverage and permutation
  significance.
  \item \textbf{Twin-based repairs:} alerted windows surface aligned precedents
  that translate into repair snippets for both planning and coding agents.
  \item \textbf{Statistical audit:} 20\,000-shuffle permutation tests quantify
  significance across guardrail settings and reveal where further work is needed.
\end{itemize}

\section{Discussion}
STM guardrails complement instruction-tuned planners by offering calibrated
coverage, actionable lead-time, and structural repair suggestions. The refreshed
feature set---effect-alignment cues in the PDDL adapter and AST-aware edit
profiles for CodeTrace---raises the signal-to-noise ratio of alerting windows.
With VAL-generated traces the logistics guardrail now fires on roughly
1.8\% of windows (weighted) while preserving five to six step leads and perfect
precision, yet permutation tails remain high ($p_{\min}\approx 0.09$). The
Blocksworld configuration does fire at the 5\% budget but does so almost exactly
at the failure boundary (mean lead $=0$) leaving $p_{\min}\approx 0.14$, while
Mystery continues to alert at 2--3\% coverage with seven-step leads but null
permutation scores. These results mirror the discussion in
Section~\ref{subsec:permutation}: richer foreground features alone are not
sufficient—the twin corpora must be scaled and the calibration loop must
optimise directly for permutation significance to escape the synthetic plateau.

These improvements matter in real deployments: Logistics-style predicates mirror
the resource and fleet constraints surfaced by critical infrastructure partners,
while AST-aware coding alerts let maintenance agents surface high-risk edits
before they land in production. Combined, the guardrail keeps foreground budgets
low enough for human-in-the-loop review while remaining sensitive to the semantic
structure of the underlying domains.

\subsection{Limitations}
Permutation $p$-values stay high at nominal coverage. With VAL traces the
logistics guardrail attains $0.018$ weighted coverage and a five-step mean lead
yet only reaches $p_{\min}=0.09$; Blocksworld delivers more alerts but with
zero-step leads and $p_{\min}=0.14$; Mystery produces seven-step leads at
${\sim}2.7\%$ coverage but $p_{\min}=0.71$. The domain-specific features improve
alignment signals but cannot compensate for the limited foreground corpora or
the deterministic corruption patterns in PlanBench. Adapters still focus on PDDL
traces and Python-heavy CodeTrace telemetry; twin corpora are curated from the
same VAL runs and a handful of maintenance tasks. Dataset scale (300 problems per
PlanBench domain, 500 for the Logistics probe, three CodeTrace tasks) limits
statistical confidence.

\subsection{Future Work}
To tighten significance and improve robustness we will:
\begin{itemize}
  \item scale PlanBench exports to 500--1000 instances per domain and continue
        diversifying CodeTrace scenarios across languages so that lower
        guardrails are exercised on longer, more varied traces;
  \item ingest real-world plan traces, robotic telemetry, and bug-fix commits
        via the new enrichment hooks (\texttt{PLANBENCH\_EXTRA\_TWINS}) to broaden the
        twin corpus beyond synthetic data;
  \item generalise the permutation optimiser so that every domain can enforce
        $p \le 0.05$ targets in-loop, including joint searches that coordinate
        foreground budgets across related corpora;
  \item continue evolving feature-level improvements (longer foreground windows,
        signature-aware twin filtering, richer semantic metrics) and repeat the
        permutation study to determine whether Blocksworld or Mystery can push
        $p_{\min}$ below $0.05$;
  \item couple guardrail optimisation with planner feedback loops so that
        permutation outcomes and twin repairs are tuned alongside instruction
        policies rather than audited post-hoc.
\end{itemize}

\section{Conclusion}
We provide a research-focused account of STM guardrails for symbolic planning
agents, delivering calibrated configurations, permutation analyses, and
reproducible scripts. The release surfaces a clear agenda: maintain low alert
budgets while strengthening statistical significance and broadening adapter
coverage. We hope this baseline informs future collaboration with the PlanBench
community and complementary instruction-tuning efforts.

\appendix

\section{Reproducibility Checklist}
Key commands are listed below; outputs are referenced throughout the text and in
\texttt{docs/tests/}.

\begin{lstlisting}[style=stm]
make planbench-all    # regenerate dataset, manifolds, guardrail sweeps
# PLANBENCH_EXTRA_TWINS="data/twins/bugfix_state.json" make planbench-all
#   (optional) merge additional gold states into Blocksworld/Mystery twins
make codetrace-report # rebuild CodeTrace comparison report
.venv/bin/pytest      # regression suite (22 passed, 1 skipped)
# PlanBench scale probes (Section~\ref{subsec:permutation})
PLANBENCH_SCALE_TARGETS="logistics blocksworld mystery_bw" make planbench-scale
# To regenerate a single domain, override PLANBENCH_SCALE_TARGETS (e.g. "logistics")
\end{lstlisting}

\begin{thebibliography}{9}
\bibitem{planbench} E. Gripper, L. Pineda, and P. Shah. \emph{PlanBench: A Benchmark Suite for Plan Validation}. MIT CSAIL Technical Report, 2023.
\bibitem{stm-manifold} SepDynamics Research. \emph{Structural Manifold Methods for Early Warning}. Internal Whitepaper, 2024.
\bibitem{verma2025pddlinstruct} P. Verma, N. La, A. Favier, S. Mishra, and J. A. Shah. \emph{Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning}. arXiv:2509.13351, 2025.
\end{thebibliography}

\end{document}
